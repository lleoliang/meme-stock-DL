# Meme Stock Surge Prediction - Stream B (Social Encoder)

A deep learning model that predicts meme stock surges (â‰¥2% price increase within 5 sessions) using social signals from Stocktwits. The model uses LSTM/Transformer encoders with attention mechanisms to learn patterns from social volume, sentiment, and velocity.

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd meme-stock-DL

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```bash
# Run complete pipeline (data collection, training, backtesting)
python main.py --mode full

# Or run individual steps
python main.py --mode collect    # Collect Stocktwits data
python main.py --mode train      # Train model
python main.py --mode backtest   # Run backtesting
```

## ğŸ“ Project Structure

```
meme-stock-DL/
â”œâ”€â”€ main.py                      # Main entry point (redirects to src.main)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # This file
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Main CLI entry point
â”‚   â”œâ”€â”€ config.py               # Configuration parameters
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # Model architecture
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ stream_b.py         # Stream B model
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                   # Data collection & processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_collector.py
â”‚   â”‚   â”œâ”€â”€ data_collector_enhanced.py
â”‚   â”‚   â”œâ”€â”€ data_processor.py
â”‚   â”‚   â”œâ”€â”€ historical_data_loader.py
â”‚   â”‚   â””â”€â”€ scrapers/
â”‚   â”‚       â”œâ”€â”€ selenium_scraper.py
â”‚   â”‚       â””â”€â”€ stocktwits_scraper.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/               # Training modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prepare_and_train.py
â”‚   â”‚   â”œâ”€â”€ train_stream_b.py
â”‚   â”‚   â””â”€â”€ losses.py
â”‚   â”‚
â”‚   â”œâ”€â”€ backtest/               # Backtesting
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ backtest.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ sentiment_analyzer.py
â”‚
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ collect_real_data.py
â”‚   â”œâ”€â”€ quick_setup.py
â”‚   â””â”€â”€ verify_format.py
â”‚
â”œâ”€â”€ tests/                       # Test files
â”‚   â””â”€â”€ test_backtest.py
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â”œâ”€â”€ QUICKSTART.md
â”‚   â””â”€â”€ SYSTEM_CONTEXT.md
â”‚
â”œâ”€â”€ data/                        # Data files
â”‚   â”œâ”€â”€ raw/                    # Raw CSV files
â”‚   â””â”€â”€ samples/                # Sample data
â”‚
â””â”€â”€ examples/                   # Example code
    â”œâ”€â”€ model.py
    â””â”€â”€ stock_data_scraper.py
```

## ğŸ¯ Features

- **Social Encoder**: LSTM/GRU/Transformer encoder for Stocktwits signals
- **Multi-source Data Collection**: API, web scraping, and historical data support
- **NLP Sentiment Analysis**: VADER and FinBERT integration
- **Weight Optimization**: Automated hyperparameter tuning for class imbalance
- **Backtesting**: Full trading simulation with transaction costs
- **Comprehensive Metrics**: PR-AUC, Precision@K, CAGR, Sharpe Ratio, Max Drawdown

## ğŸ“Š Model Architecture

**Stream B** consists of:

1. **Social Encoder**: Processes sequences of [volume, sentiment, velocity] with shape [N, 60, 3]
2. **Attention Pooling**: Self-attention mechanism for sequence aggregation
3. **Classifier**: Multi-layer MLP for binary surge prediction

```
Input: S_t âˆˆ R^(60 x 3) = [volume, sentiment, velocity]
  â†“
LSTM Encoder â†’ [B, 60, H]
  â†“
Attention Pooling â†’ [B, H]
  â†“
MLP Classifier â†’ [B] logits
```

## âš™ï¸ Configuration

Edit `config.py` to adjust:

- `SEQUENCE_LENGTH`: Time window size (default: 60)
- `HIDDEN_DIM`: Model hidden dimension (default: 64)
- `SURGE_THRESHOLD_PCT`: Price increase threshold (default: 2%)
- `SURGE_FORWARD_WINDOW`: Prediction horizon (default: 5 sessions)
- `TOP_K_PREDICTIONS`: Number of stocks to trade (default: 10)

## ğŸ“ˆ Data Collection

The system supports multiple data collection methods:

1. **Stocktwits API** (requires authentication)
2. **Web Scraping** (Selenium-based)
3. **Historical Data** (CSV files)

Data is automatically aggregated into daily features:
- **Volume**: Message count per day
- **Sentiment**: Average sentiment score (-1 to 1)
- **Velocity**: Rate of change in message volume

## ğŸ‹ï¸ Training

The training pipeline includes:

- **Class Imbalance Handling**: Weighted BCE loss with optimized weights
- **Early Stopping**: Prevents overfitting
- **Gradient Clipping**: Stabilizes training
- **Metrics**: PR-AUC (primary), Precision@K

```bash
python prepare_and_train.py
```

This will:
1. Collect/load Stocktwits data
2. Process and align with market data
3. Optimize class weights via grid search
4. Train the final model with optimal weights
5. Run backtesting

## ğŸ“‰ Backtesting

The backtesting framework simulates trading:

- **Strategy**: Top-K stocks by prediction probability
- **Position Management**: Equal-weighted, hold for forward_window days
- **Transaction Costs**: 0.1% per trade
- **Metrics**: CAGR, Sharpe Ratio, Max Drawdown, Win Rate

```bash
python main.py --mode backtest
```

Results are saved to `results/`:
- `backtest_results.json`: Performance metrics
- `backtest_trades.csv`: Individual trade log
- `backtest_plots.png`: Visualization

## ğŸ”§ Advanced Usage

### Custom Symbols

```bash
python main.py --mode collect --symbols GME TSLA AMC
```

### Load Pre-trained Model

```python
import torch
from models.stream_b import StreamBClassifier
from config import Config

checkpoint = torch.load('models/stream_b_best.pth')
model = StreamBClassifier(
    input_dim=Config.SOCIAL_FEATURE_DIM,
    hidden_dim=checkpoint['config']['hidden_dim'],
    num_layers=checkpoint['config']['num_layers'],
    dropout=checkpoint['config']['dropout']
)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

### Data Collection Methods

The enhanced collector tries multiple methods in order:
1. Stocktwits API (if authenticated)
2. Selenium scraper
3. Historical data loader
4. Raises error (synthetic data disabled)

## ğŸ“ Notes

- **Academic Use Only**: This is for research/backtesting purposes
- **API Limitations**: Stocktwits API has rate limits and may require authentication
- **Data Quality**: Ensure sufficient data points per ticker (min: 30 days)
- **Stocktwits Blocking**: Stocktwits actively blocks automated access; use historical data or manual collection

## ğŸ› Troubleshooting

### Stocktwits API Issues

If you encounter 403 errors:
- Stocktwits has paused new API registrations
- Use historical data files in `data/historical/`
- Try Selenium scraper (may be blocked)
- Consider alternative data sources (Reddit, Twitter)

### Selenium Issues

If Selenium fails:
- Ensure Chrome browser is installed
- Install: `pip install selenium webdriver-manager`
- Try non-headless mode: Set `headless=False` in `selenium_scraper.py`

### Data Issues

If no data is found:
- Check `data/raw/` directory for CSV files
- Ensure CSV format: `date,volume,sentiment,velocity`
- Use `collect_real_data.py` to test data collection

## ğŸ“š Dependencies

See `requirements.txt` for full list. Key dependencies:

- `torch>=2.0.0`: Deep learning framework
- `pandas>=2.0.0`: Data manipulation
- `yfinance>=0.2.28`: Market data
- `selenium>=4.15.0`: Web scraping
- `vaderSentiment>=3.3.2`: Sentiment analysis

## ğŸ¤ Contributing

Contributions welcome! Please ensure:
- Code follows PEP 8 style guide
- Add tests for new features
- Update documentation

## ğŸ“„ License

This project is for educational/research purposes only.

## ğŸ™ Acknowledgments

- Stocktwits for social data
- Yahoo Finance for market data
- VADER Sentiment for NLP analysis
